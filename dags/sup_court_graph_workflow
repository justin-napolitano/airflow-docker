from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
from neo4j import GraphDatabase

# Define Neo4j connection details
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "your_password"

def read_query_file(filepath):
    with open(filepath, 'r') as file:
        query = file.read()
    return query

def ingest_data():
    sql_query = read_query_file('/home/cobra/Repos/justin-napolitano/airflow-docker/sql/ingest_data.sql')
    # Execute your SQL query here using your database connection
    print(sql_query)  # Replace this with actual query execution logic

def transform_data():
    sql_query = read_query_file('/home/cobra/Repos/justin-napolitano/airflow-docker/sql/transform_data.sql')
    # Execute your SQL query here using your database connection
    print(sql_query)  # Replace this with actual query execution logic

def run_neo4j_query():
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    cypher_query = read_query_file('/home/cobra/Repos/justin-napolitano/airflow-docker/sql/ctrbr_to_sbjt.cql')

    def execute_query(tx):
        result = tx.run(cypher_query)
        for record in result:
            print(record)

    with driver.session() as session:
        session.write_transaction(execute_query)
    driver.close()

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
}

dag = DAG('neo4j_workflow', default_args=default_args, schedule_interval='@daily')

t1 = PythonOperator(task_id='ingest_data', python_callable=ingest_data, dag=dag)
t2 = PythonOperator(task_id='transform_data', python_callable=transform_data, dag=dag)
t3 = PythonOperator(task_id='run_neo4j_query', python_callable=run_neo4j_query, dag=dag)

t1 >> t2 >> t3
